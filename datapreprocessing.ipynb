{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences as Pad\n",
    "import re\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!', 'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!', 'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.']\n",
      "[\"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\", \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L198', 'L199']\", \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L200', 'L201', 'L202', 'L203']\"]\n"
     ]
    }
   ],
   "source": [
    "# load the data in 'movie_lines.txt' and the data in 'movie_conversations.txt'\n",
    "# we will use the lineID sets in movie_convs to reconstruct the real conversations in movie_lines\n",
    "\n",
    "movie_lines = open('movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "movie_convs = open('movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "\n",
    "print(movie_lines[:3])\n",
    "print(movie_convs[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the lineID-sentence pairs, each lineID map a particular sentence\n",
    "id2sentence = {}\n",
    "for line in movie_lines:\n",
    "    line = line.split(' +++$+++ ') # split movie_lines with '+++$+++' so that we can get a list only contains informations\n",
    "    if len(line) == 5:\n",
    "        id2sentence[line[0]] = line[4] # the 1st element in list is lineID, the last element is the sentence\n",
    "        \n",
    "print(id2sentence['L1045'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a list containing all lineID sets\n",
    "conv_ID = []\n",
    "for lineIDs in movie_convs:\n",
    "    lineIDs= lineIDs.split(' +++$+++ ')[-1][1:-1].replace(' ','').replace(\"'\",\"\") # remove unnecessary symbols\n",
    "    conv_ID.append(lineIDs.split(','))\n",
    "print(conv_ID[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify questions and answers\n",
    "\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "for lineIDs in conv_ID:\n",
    "    for i in range(len(lineIDs) - 1):\n",
    "        questions.append(id2sentence[lineIDs[i]])\n",
    "        answers.append(id2sentence[lineIDs[i + 1]])\n",
    "\n",
    "print(questions[0])\n",
    "print(answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "\n",
    "    # remove unnecessary characters in sentences\n",
    "    \n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"there's\", \"there is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r'[\" \"]+', \" \", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_questions = []\n",
    "for question in questions: \n",
    "    clean_questions.append(clean_text(question))\n",
    "    \n",
    "clean_answers = []\n",
    "for answer in answers:\n",
    "    answer = 'bos ' + clean_text(answer) +' eos' # add bos (beginning of sentence) and eos (end of sentence) to answers\n",
    "    clean_answers.append(answer)\n",
    "\n",
    "print(clean_questions[0])\n",
    "print(clean_answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('number of questions: ' + str(len(clean_questions)))\n",
    "print('number of answers: ' + str(len(clean_answers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make the model easier to experiment, we remove some queries and replies that are too long or too short\n",
    "\n",
    "minlen = 2\n",
    "maxlen = 20\n",
    "filtered_questions_temp = []\n",
    "filtered_answers_temp = []\n",
    "\n",
    "# filter the Q&A pairs which question are too long or too short\n",
    "for i,question in enumerate(clean_questions):\n",
    "    if len(question.split()) <= maxlen and len(question.split()) >= minlen:\n",
    "        filtered_questions_temp.append(question)\n",
    "        filtered_answers_temp.append(clean_answers[i])\n",
    "        \n",
    "filtered_questions = []\n",
    "filtered_answers = []\n",
    "\n",
    "# from 'filtered_questions_temp' and 'filtered_answers_temp' filter the pairs that answers are too long or too short\n",
    "for i, answer in enumerate(filtered_answers_temp):\n",
    "    if len(answer.split()) <= maxlen and len(answer.split()) >= minlen:\n",
    "        filtered_answers.append(answer)\n",
    "        filtered_questions.append(filtered_questions_temp[i])\n",
    "        \n",
    "print(len(filtered_answers))\n",
    "print(len(filtered_questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_answers[0])\n",
    "print(filtered_questions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the input index sequence & vocabulary; build the output index sequence & vocabulary\n",
    "\n",
    "# we build 2 different vocabulary because it is convinient, some other code only use one\n",
    "# this method is also used in machine translation model because we need 2 different vocabulary in different language\n",
    "\n",
    "# define the tokenizer\n",
    "vocabsize = 2500\n",
    "\n",
    "# vocabulary size is 8000, and we use UNK to replace those words are not frequently used\n",
    "question_tokenizer = Tokenizer(num_words = vocabsize+1, oov_token = 'unk')\n",
    "answer_tokenizer = Tokenizer(num_words = vocabsize+1,oov_token = 'unk')\n",
    "\n",
    "# tokenize the questions and answers\n",
    "question_tokenizer.fit_on_texts(filtered_questions)\n",
    "answer_tokenizer.fit_on_texts(filtered_answers)\n",
    "\n",
    "# build the input sequence and output sequence\n",
    "q_sequences = question_tokenizer.texts_to_sequences(filtered_questions)\n",
    "a_sequences = answer_tokenizer.texts_to_sequences(filtered_answers)\n",
    "\n",
    "# pad sequences in same length so that we can train them in model\n",
    "q_pad = Pad(q_sequences, padding = 'post')\n",
    "a_pad = Pad(a_sequences, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the preprocessed data\n",
    "q_token_json = question_tokenizer.to_json()\n",
    "a_token_json = answer_tokenizer.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('preprocessed_data/questions.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(q_token_json, ensure_ascii=False))\n",
    "    f.close()\n",
    "\n",
    "with open('preprocessed_data/answers.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(a_token_json, ensure_ascii=False))\n",
    "    f.close()\n",
    "\n",
    "np.savez('preprocessed_data/data.npz', q_pad, a_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
